---
stepsCompleted: [1, 2, 3, 4, 5, 6]
status: complete
inputDocuments:
  - PLAN.md
date: 2026-02-11
author: Benji
---

# Product Brief: agent-test-kit

<!-- Content will be appended sequentially through collaborative workflow steps -->

## Executive Summary

cc-eval is a CLI-first evaluation framework that brings measurable, data-driven feedback to the Claude Code tooling ecosystem. Today, developers craft skills, rule files, sub-agents, MCP server configurations, CLAUDE.md files, and complex workflows to shape Claude Code's output — but have no systematic way to know if any of it actually works. Iteration is driven by gut feel: tweak a prompt, eyeball the result, repeat.

cc-eval replaces this with a structured feedback loop: define a requirement with acceptance criteria, run Claude Code with a given configuration, automatically evaluate the output across five dimensions (requirement fulfillment, code quality, functional correctness, tool usage, and efficiency), and get comparable scores. A/B test different configurations — the same task run with a skill vs. a rule vs. a CLAUDE.md entry — and let the numbers tell you which approach works best.

Born from real patterns observed across many developers in AI-assisted development training, cc-eval is the first purpose-built framework for fine-tuning how developers use Claude Code to achieve their goals.

---

## Core Vision

### Problem Statement

When building software with Claude Code, developers invest significant effort crafting an ecosystem of tooling: rule files that define coding patterns, skills for repeatable workflows, sub-agents for specialized tasks, MCP servers for external integrations, and CLAUDE.md files for project context. This tooling directly shapes the quality of code Claude Code produces — but there is no way to measure whether it's actually effective.

The entire feedback loop is broken. When Claude Code doesn't meet expectations, the best available advice is "try another prompt" or "rewrite the skill." There's no way to know what to change, whether a change helped, or by how much. Developers iterate blindly, guided by intuition rather than evidence.

### Problem Impact

- **Wasted iteration time** — Developers spend hours tweaking configurations with no measurable signal on whether changes improved outcomes
- **Invisible tool underutilization** — A developer may build a well-crafted testing skill that Claude Code never invokes, and they'd never know
- **No reproducibility** — Without structured evaluation, there's no way to compare configurations or share best practices with evidence
- **Scaling knowledge is impossible** — Trainers and teams cannot systematically identify what makes a Claude Code setup effective vs. ineffective

### Why Existing Solutions Fall Short

- **Generic LLM evaluation frameworks** (e.g., RAGAS-based tools) evaluate text responses from API calls — not code generated by an agentic, multi-turn, tool-using coding assistant
- **Code quality tools** (ESLint, SonarQube) measure the code itself but tell you nothing about whether the tooling that produced the code was effective or optimally configured
- **Manual review** — asking Claude Code "why didn't you do X?" — is unreliable, unreproducible, and doesn't scale
- **No purpose-built solution exists** for evaluating and fine-tuning AI coding agent configurations

### Proposed Solution

cc-eval is a testing framework for your Claude Code setup — vitest/pytest but for your tooling ecosystem. It provides:

1. **Define** — Write test suites in YAML with prompts and acceptance criteria describing what Claude Code should build
2. **Execute** — Run Claude Code in isolated workspaces via the Claude Agent SDK, capturing full session transcripts
3. **Evaluate** — Automatically score output across five orthogonal dimensions: requirement fulfillment (LLM-as-judge), code quality (static analysis), functional correctness (build/test), tool usage analysis (LLM-as-judge), and efficiency (session telemetry)
4. **Compare** — A/B test different configurations side-by-side with measurable deltas
5. **Iterate** — Use the data to refine skills, rules, sub-agents, and workflows with confidence

### Key Differentiators

- **First of its kind** — No purpose-built framework exists for evaluating AI coding agent configurations. cc-eval fills a genuine gap in the ecosystem
- **A/B testing of configurations** — Run the same task with different setups and compare numbers, not feelings. Test whether a prompt works better as a skill, a rule, or a CLAUDE.md entry
- **Tool usage intelligence** — Goes beyond "did the code work" to analyze whether Claude Code effectively leveraged the available tooling, surfacing missed opportunities and underutilized resources
- **Feelings to numbers** — Replaces subjective gut-feel iteration with reproducible, measurable, comparable data across five evaluation dimensions
- **Practitioner-designed** — Built from patterns observed across many developers in real AI-assisted development training, addressing problems that surface repeatedly in practice

---

## Target Users

### Primary Users

**Any developer using Claude Code who invests in customizing their tooling ecosystem.**

This includes solo developers and teams, from intermediate to elite level, across all specializations — full-stack, backend, frontend, DevOps, or any other discipline. The common thread is not their experience level or tech stack, but their behavior: they actively craft and iterate on skills, rule files, sub-agents, MCP configurations, CLAUDE.md files, and workflows to improve Claude Code's output.

**Persona: The Prompt Iterator**

- **Name:** Alex — a full-stack developer who uses Claude Code daily for building software
- **Context:** Alex has built several skills (testing patterns, frontend development, code review), maintains rule files for coding standards, and has configured sub-agents for specialized tasks. Alex treats the Claude Code tooling ecosystem as a first-class part of their development setup
- **Day-to-day:** Alex writes code with Claude Code, tweaks prompts when results aren't right, rewrites skills when they feel ineffective, and experiments with different approaches (should this be a rule or a skill? should the CLAUDE.md include more context?)
- **The frustration:** Alex keeps iterating — trying different prompt structures, rewriting skills, adjusting configurations — but has no data on whether any of it is actually improving outcomes. It's all gut feel. There's no "before vs. after" comparison, no score to track, no signal to follow. The iteration loop feels endless and directionless
- **The success moment with cc-eval:** Alex rewrites a testing skill, runs cc-eval against the same task with both versions, and sees: the new skill scores 92% requirement fulfillment vs. 71% before, and Claude Code actually invoked the skill in 100% of runs instead of 60%. For the first time, Alex has *proof* that the change worked — and knows exactly where to focus next

### Secondary Users

Not defined for MVP scope. Future potential includes: trainers demonstrating best practices with measurable evidence, team leads standardizing Claude Code configurations across teams, and broader generalization beyond code evaluation to measure goal achievement for any Claude Code task.

### User Journey

1. **Discovery:** Developer hears about cc-eval through the Claude Code community, training, or documentation. The pitch resonates immediately: "measure whether your Claude Code setup actually works"
2. **Onboarding:** `npm install -D cc-eval && npx cc-eval init` — creates config and an example test suite in under a minute. Developer writes their first test YAML with a real task and acceptance criteria from their project
3. **First run:** `npx cc-eval run` — Claude Code executes the task in an isolated workspace, and cc-eval displays scores across all five dimensions. Developer sees concrete numbers for the first time
4. **Aha! moment:** Developer tweaks a skill file, runs cc-eval again, and sees the requirement fulfillment score jump from 75% to 90%. The tool usage report shows Claude Code now consistently calls the skill. *"So that's what was wrong."*
5. **Core usage:** cc-eval becomes part of the developer's iteration loop. Before committing to a tooling change, they run cc-eval to verify it actually improves outcomes. A/B testing different configurations becomes routine
6. **Long-term:** The developer builds a library of test suites covering their most common tasks. Their Claude Code setup steadily improves based on data. Prompt engineering becomes a measurable craft, not guesswork

---

## Success Metrics

### User Success Metrics

Success for cc-eval is measured by whether developers gain a measurable grip on their Claude Code tooling ecosystem — replacing blind iteration with data-driven improvement.

**Core User Outcomes:**

- **Time saved** — Developers reduce the time spent blindly iterating on prompts, skills, and configurations by having clear signals on what works and what doesn't
- **Numbers-backed feedback** — Every tooling change produces comparable, reproducible scores across five evaluation dimensions. Developers no longer rely on gut feel
- **Sparring partner insight** — The LLM-as-judge evaluation provides actionable feedback on both the output quality and how effectively Claude Code used the available tooling, acting as an expert review partner
- **Systematic improvement** — Developers can follow a structured loop: change configuration, run evaluation, compare results, iterate with confidence. Progress is visible and trackable

**User Success Indicator:**
A developer says: *"This is the first time I have a grip on what effect my changes in prompts and tooling have, because I can measure it and systematically improve it."*

### Business Objectives

cc-eval is developed for internal use within the company. No commercial business objectives are defined at this stage. The tool serves as an internal capability accelerator — improving how the team and training participants use Claude Code for software development.

Future potential: If the tool proves valuable internally, open-sourcing or commercialization may be considered, but this is explicitly out of scope for now.

### Key Performance Indicators

**Product Effectiveness KPIs:**

- **Evaluation accuracy** — LLM-as-judge assessments align with human judgment when spot-checked (qualitative validation during internal use)
- **Actionable signal rate** — Evaluation results lead to a concrete tooling change (not just "interesting numbers" but numbers that drive action)
- **A/B test clarity** — When comparing two configurations, the scores produce a clear directional signal (one is measurably better than the other)

**User Adoption KPIs (internal):**

- **Repeat usage** — Developers run cc-eval more than once (indicating it's useful enough to come back to)
- **Configuration improvements** — Developers make tooling changes based on cc-eval results and see improved scores on subsequent runs
- **Test suite growth** — Developers create additional test suites beyond the initial example, covering more of their real tasks

---

## MVP Scope

### Core Features

**The MVP delivers a complete evaluate-and-compare loop: run Claude Code against a defined requirement, score the output, and compare configurations.**

**1. CLI Foundation**
- `cc-eval init` — Initialize project with config file, test directory, and example test suite
- `cc-eval run [test-name]` — Execute test suite(s) with full evaluation pipeline
- `cc-eval compare <run1> <run2>` — Side-by-side comparison of two runs with delta highlighting
- `cc-eval list` — List past runs with summary

**2. Configuration Layer**
- YAML-based project config (`cc-eval.config.yaml`) with Zod validation
- YAML-based test suite definitions with prompts and acceptance criteria
- Test suite config overrides project-level defaults

**3. Workspace Isolation**
- Git worktree-based isolated workspaces for each test run
- Fallback to directory copy for non-git repositories
- Automatic cleanup after runs

**4. Claude Code Execution**
- Programmatic execution via Claude Agent SDK (`@anthropic-ai/claude-agent-sdk`)
- Full session transcript capture (tool calls, responses, errors, metadata)
- Configurable model and max turns per test suite

**5. Evaluation Metrics (3 of 5 dimensions)**
- **Efficiency** — Tokens, cost, turn count, duration, tool call counts by type, error count. Computed directly from session telemetry
- **Requirement Fulfillment** — LLM-as-judge evaluates generated code against each acceptance criterion. Per-criterion PASS/FAIL with reasoning, overall percentage score
- **Tool Usage Analysis** — LLM-as-judge reviews session transcript against available tools manifest (contents of `.claude/`). Identifies tools used (with frequency), tools that should have been used but weren't, and overall utilization effectiveness

**6. LLM Judge**
- Anthropic SDK wrapper for judge calls with structured output parsing
- Configurable judge model (defaults to Claude Sonnet)
- Retries and error handling
- Shared infrastructure for both requirement fulfillment and tool usage analysis

**7. Reporting & Storage**
- Terminal output with colored, formatted results (vitest-style)
- JSON persistence to `.cc-eval/runs/<run-id>/`
- Compare view with metric deltas (`+12.5%` requirement fulfillment, `-3 turns`, etc.)

### Out of Scope for MVP

- **Code Quality metric** (static analysis / linters) — Deferred to post-MVP
- **Functional Correctness metric** (build/test/coverage) — Deferred to post-MVP
- **Manual evaluation mode** (evaluate pre-existing code without running Claude Code) — Deferred
- **HTML report generation** — Deferred; terminal + JSON is sufficient for MVP
- **Config overlay via CLI flag** (A/B testing with alternative `.claude/` directories) — Deferred; users can manually swap configs and compare runs in MVP
- **Statistical multi-run analysis** (aggregate trends across many runs) — Future vision
- **Web dashboard** — Future vision

### MVP Success Criteria

- A developer can define a test suite, run it, and see requirement fulfillment and tool usage scores in the terminal
- A developer can run the same test with two different tooling configurations and use `cc-eval compare` to see which performed better with clear numerical deltas
- The tool usage report identifies when Claude Code failed to invoke available skills or rules, giving developers actionable insight into their configuration
- Results are persisted and reproducible — running `cc-eval list` shows history, runs can be compared at any time
- The evaluation loop (change config → run → score → compare) completes in a reasonable time for a typical task

### Future Vision

- **Full 5-dimension evaluation** — Add code quality and functional correctness metrics to provide comprehensive scoring
- **Config overlay system** — CLI flag to swap `.claude/` configurations for streamlined A/B testing
- **HTML reports** — Rich, shareable report generation for team collaboration
- **Statistical analysis** — Aggregate trends across multiple runs to identify patterns (e.g., "this skill is only invoked 20% of the time across 100 runs")
- **Manual evaluation mode** — Evaluate pre-existing codebases without running Claude Code
- **Generalization beyond code** — Extend the framework to measure goal achievement for any Claude Code task, not just code generation
- **Team features** — Shared test suites, configuration libraries, and benchmark comparisons across team members
